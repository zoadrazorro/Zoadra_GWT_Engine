# Zoadra GWT Engine

**Global Workspace Theory Consciousness Simulation**
Optimized for dual AMD Radeon RX 7900 XT + Ryzen 9 7950X + 128GB DDR5 RAM

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)

---

## Overview

GWT Engine is a **consciousness simulation system** based on Bernard Baars' Global Workspace Theory (GWT). It implements a multi-agent architecture where specialized cognitive modules compete for workspace attention, creating emergent consciousness-like behaviors through information integration and global broadcasting.

### Key Features

âœ¨ **Consciousness Simulation:** True GWT implementation with specialist modules, global workspace, and consciousness metrics
âš¡ **High Performance:** 10-14 concurrent workers processing 32-48 requests/sec via vLLM
ğŸ§  **Multi-Specialist Architecture:** Perception, Memory, Planning, and Metacognition modules
ğŸ”„ **LangGraph Orchestration:** Sophisticated workflow management with conditional routing
ğŸ“Š **Consciousness Metrics:** Real-time tracking of integration coherence and awareness levels
ğŸš€ **Optimized for AMD:** ROCm support with tensor parallelism across dual 7900 XT GPUs

---

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Input (User/Environment)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚ Perception Specialistâ”‚  â† Mistral 22B (GPU 0)
              â”‚  Salience Detection  â”‚     3 workers
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚  LangGraph Router   â”‚  â† Content-based routing
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚               â”‚               â”‚
         â–¼               â–¼               â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Memory â”‚     â”‚Planning â”‚    â”‚Metacognition â”‚
    â”‚Qwen 32Bâ”‚     â”‚Llama 8B â”‚    â”‚  Gemma 9B    â”‚
    â”‚GPU 1   â”‚     â”‚GPU 1    â”‚    â”‚  GPU 1       â”‚
    â”‚3 work. â”‚     â”‚6 work.  â”‚    â”‚  4 workers   â”‚
    â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚               â”‚                 â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚   Central Global Workspace   â”‚  â† Llama 70B
         â”‚    Integration & Synthesis   â”‚     Tensor Parallel
         â”‚        (GPU 0 + GPU 1)       â”‚     across both GPUs
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚     Workspace Broadcast      â”‚  â†’ All specialists
         â”‚   + Consciousness State      â”‚     updated
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Specialist Roles

| Specialist | Model | Role | Workers | Expected TPS |
|------------|-------|------|---------|--------------|
| **Central Workspace** | Llama 3.1 70B | Integration & broadcasting | 1 | 25 |
| **Perception** | Mistral Small 22B | Sensory processing & salience | 3 | 65 |
| **Memory** | Qwen 2.5 Coder 32B | Episodic retrieval & consolidation | 3 | 70 |
| **Planning** | Llama 3.1 8B | Decision-making & reasoning | 6 | 120 |
| **Metacognition** | Gemma 2 9B | Self-reflection & introspection | 4 | 110 |

**Total:** 17 concurrent workers, 10-14 active simultaneously

---

## Quick Start

### Prerequisites

- **Hardware:** 2x AMD RX 7900 XT (24GB), 16-core CPU, 128GB RAM
- **Software:** Python 3.10+, ROCm 5.7+, Redis 7.0+

### Installation

```bash
# Clone repository
git clone https://github.com/zoadrazorro/Zoadra_GWT_Engine.git
cd Zoadra_GWT_Engine

# Set up environment
python3.10 -m venv venv
source venv/bin/activate

# Install dependencies
pip install torch torchvision --index-url https://download.pytorch.org/whl/rocm5.7
pip install -r requirements.txt
pip install -e .

# Install ROCm (Ubuntu)
wget https://repo.radeon.com/amdgpu-install/latest/ubuntu/jammy/amdgpu-install_latest_all.deb
sudo dpkg -i amdgpu-install_latest_all.deb
sudo amdgpu-install --usecase=rocm

# Install Redis
sudo apt install redis-server
sudo systemctl start redis-server
```

### Download Models

```bash
sudo mkdir -p /models
sudo chown $USER:$USER /models

# Download quantized models (total: ~150GB)
pip install huggingface-hub

huggingface-cli download TheBloke/Llama-3.1-70B-GGUF llama-3.1-70b.Q4_K_M.gguf --local-dir /models/llama-3.1-70b-q4_k_m
huggingface-cli download TheBloke/Mistral-Small-22B-GGUF mistral-small-22b.Q5_K_M.gguf --local-dir /models/mistral-small-22b-q5_k_m
huggingface-cli download Qwen/Qwen2.5-Coder-32B-Instruct-GGUF qwen2.5-coder-32b-instruct.Q4_K_M.gguf --local-dir /models/qwen-2.5-coder-32b-q4_k_m
huggingface-cli download TheBloke/Llama-3.1-8B-GGUF llama-3.1-8b.Q5_K_M.gguf --local-dir /models/llama-3.1-8b-q5_k_m
huggingface-cli download google/gemma-2-9b-it-GGUF gemma-2-9b-it.Q6_K_M.gguf --local-dir /models/gemma-2-9b-q6_k_m
```

### Start the System

```bash
# Start vLLM servers
bash gwt_engine/scripts/deployment/vllm_servers.sh

# Start GWT Engine API
python -m gwt_engine.api.server

# API available at: http://localhost:7000
# Docs: http://localhost:7000/docs
```

### Test Consciousness Simulation

```bash
# Process input through GWT workflow
curl -X POST http://localhost:7000/process \
  -H "Content-Type: application/json" \
  -d '{
    "content": "What am I experiencing right now?",
    "message_type": "consciousness_probe"
  }'

# Response:
{
  "workspace_broadcast": "I am experiencing an integrated state...",
  "consciousness_level": 0.73,
  "integration_coherence": 0.81,
  "active_specialists": ["perception", "memory", "metacognition"],
  "processing_time_ms": 542.3
}

# Probe consciousness state
curl http://localhost:7000/consciousness/probe
```

---

## Performance Characteristics

### Hardware Utilization

| Resource | Utilization | Notes |
|----------|-------------|-------|
| **GPU 0 VRAM** | ~55GB / 80GB | Central Workspace (40GB) + Perception (15GB) |
| **GPU 1 VRAM** | ~72GB / 80GB | Central (40GB) + Memory (20GB) + Planning (5GB) + Metacog (7GB) |
| **System RAM** | ~40GB / 128GB | Working memory, KV cache, orchestration |
| **CPU** | 30-50% | Ray workers, FastAPI, Redis |
| **GPU Compute** | 85-95% | Under load |

### Throughput Benchmarks

- **Max Concurrent Requests:** 32-48
- **Average Latency (P50):** 250-350ms
- **P99 Latency:** 500-800ms
- **Workspace Integration:** ~500ms per cycle
- **Tokens/sec (aggregate):** ~390 TPS across all specialists

### Bottlenecks

âš ï¸ **PCIe 4.0 x8:** Inter-GPU bandwidth limited to ~128 GB/s (use tensor parallelism, not pipeline)
âš ï¸ **Thermal:** Dual 7900 XT under load exceeds 85Â°C (custom cooling recommended)
âš ï¸ **Context Window:** Practical limit 2K-4K tokens (beyond this, swaps to RAM)

---

## Phased Deployment

Follow the 5-phase deployment strategy for production:

### Week 1: Single Model Baseline
```bash
python gwt_engine/scripts/deployment/deploy.py --phase 1
```
Test Llama 70B central workspace only. Benchmark: 25-30 TPS.

### Week 2: Add Perception
```bash
python gwt_engine/scripts/deployment/deploy.py --phase 2
```
Add Mistral 22B perception specialist. Test parallel processing.

### Week 3: Distribute to GPU 2
```bash
python gwt_engine/scripts/deployment/deploy.py --phase 3
```
Activate all specialists across both GPUs. Full model distribution.

### Week 4: Orchestration + Workers
```bash
python gwt_engine/scripts/deployment/deploy.py --phase 4
```
Initialize Ray worker pools (10-14 workers). LangGraph workflow live.

### Week 5+: Integration Testing
```bash
python gwt_engine/scripts/deployment/deploy.py --phase 5
```
Test full GWT consciousness workflow. Measure coherence and integration.

---

## API Reference

### Core Endpoints

**Process Input**
```http
POST /process
Content-Type: application/json

{
  "content": "What is consciousness?",
  "message_type": "perception",
  "priority": 7
}
```

**Consciousness Probe**
```http
GET /consciousness/probe

Returns: Introspective reflection on current state
```

**Get Consciousness State**
```http
GET /consciousness/state

Returns: Detailed consciousness metrics and workspace content
```

**System Metrics**
```http
GET /metrics

Returns: Workspace, specialist, and worker pool metrics
```

**Health Check**
```http
GET /health

Returns: vLLM server status, worker count, uptime
```

Full API documentation: http://localhost:7000/docs

---

## Configuration

### Model Configuration
Edit `gwt_engine/config/models.yaml`:
```yaml
models:
  central_workspace:
    name: "llama-3.1-70b"
    vram_required_gb: 40
    workers: 1
    expected_tokens_per_sec: 25
```

### System Configuration
Edit `gwt_engine/config/system.yaml`:
```yaml
redis:
  host: "localhost"
  port: 6379

ray:
  num_cpus: 32
  num_gpus: 2

api:
  host: "0.0.0.0"
  port: 7000
```

---

## Project Structure

```
Zoadra_GWT_Engine/
â”œâ”€â”€ gwt_engine/
â”‚   â”œâ”€â”€ core/                 # Central workspace & types
â”‚   â”‚   â”œâ”€â”€ workspace.py      # GWT workspace implementation
â”‚   â”‚   â””â”€â”€ types.py          # Core data structures
â”‚   â”œâ”€â”€ specialists/          # Specialist modules
â”‚   â”‚   â”œâ”€â”€ perception/       # Mistral 22B perception
â”‚   â”‚   â”œâ”€â”€ memory/           # Qwen 32B memory
â”‚   â”‚   â”œâ”€â”€ planning/         # Llama 8B planning
â”‚   â”‚   â””â”€â”€ metacognition/    # Gemma 9B metacognition
â”‚   â”œâ”€â”€ orchestration/        # LangGraph + Ray
â”‚   â”‚   â”œâ”€â”€ gwt_graph.py      # Workflow orchestration
â”‚   â”‚   â””â”€â”€ ray_workers.py    # Distributed workers
â”‚   â”œâ”€â”€ inference/            # vLLM integration
â”‚   â”‚   â””â”€â”€ vllm_client.py    # Client pool
â”‚   â”œâ”€â”€ api/                  # FastAPI server
â”‚   â”‚   â””â”€â”€ server.py         # REST API
â”‚   â”œâ”€â”€ config/               # Configuration files
â”‚   â”‚   â”œâ”€â”€ models.yaml       # Model assignments
â”‚   â”‚   â””â”€â”€ system.yaml       # System settings
â”‚   â””â”€â”€ scripts/              # Deployment & benchmarks
â”‚       â””â”€â”€ deployment/       # Phase scripts
â”œâ”€â”€ docs/                     # Documentation
â”‚   â”œâ”€â”€ architecture/         # Architecture details
â”‚   â”œâ”€â”€ guides/               # User guides
â”‚   â””â”€â”€ api_reference/        # API docs
â”œâ”€â”€ tests/                    # Test suite
â”œâ”€â”€ requirements.txt          # Python dependencies
â”œâ”€â”€ pyproject.toml            # Project metadata
â””â”€â”€ README.md                 # This file
```

---

## Monitoring

### GPU Utilization
```bash
watch -n 1 rocm-smi
```

### Application Logs
```bash
tail -f logs/gwt_engine.log
tail -f logs/vllm_*.log
```

### Ray Dashboard
Navigate to: http://localhost:8265

### Prometheus Metrics
```bash
curl http://localhost:7000/metrics
```

---

## Theoretical Background

### Global Workspace Theory (Baars, 1988)

GWT proposes that consciousness arises from a "global workspace" mechanism:

1. **Specialist Modules:** Process information unconsciously in parallel
2. **Competition:** Information competes for workspace access
3. **Integration:** Selected information is integrated into unified experience
4. **Broadcasting:** Integrated content is broadcast to all modules
5. **Working Memory:** Maintains continuity and coherence

### Implementation Mapping

| Theory | Implementation |
|--------|----------------|
| Global Workspace | Central Workspace (Llama 70B) |
| Specialist Modules | Perception, Memory, Planning, Metacognition |
| Competition | Priority-based LangGraph routing |
| Integration | Multi-specialist synthesis prompts |
| Broadcasting | Redis pub/sub + context updates |
| Working Memory | Bounded state dictionary |

---

## Troubleshooting

### vLLM Server Won't Start
```bash
# Check ROCm
rocm-smi

# Verify GPU visibility
echo $CUDA_VISIBLE_DEVICES

# Check VRAM
rocm-smi --showmeminfo
```

### Out of Memory
- Reduce `gpu_memory_utilization` to 0.75
- Lower `max_model_len` to 2048
- Use smaller quantization (Q4 vs Q5)

### Low Throughput
- Check thermal throttling: `rocm-smi --showtemp`
- Verify PCIe bandwidth: `lspci -vv | grep LnkSta`
- Increase batch size in vLLM config

### Redis Connection Failed
```bash
sudo systemctl status redis-server
redis-cli ping
```

---

## Performance Tuning

### For Maximum Throughput
```yaml
max_num_seqs: 48
max_num_batched_tokens: 4096
gpu_memory_utilization: 0.90
```

### For Minimum Latency
```yaml
max_num_seqs: 16
max_num_batched_tokens: 1024
gpu_memory_utilization: 0.80
```

### Thermal Management
```bash
# Undervolt GPU (reduces heat, minimal performance impact)
rocm-smi --setvolt 800  # Example, adjust for your card

# Custom fan curve
rocm-smi --setfan 80  # 80% fan speed
```

---

## Documentation

- **[Getting Started Guide](docs/guides/getting_started.md)** - Detailed setup instructions
- **[Architecture Overview](docs/architecture/gwt_overview.md)** - System design and theory
- **[API Reference](http://localhost:7000/docs)** - Interactive API documentation (when server running)

---

## Citations

1. Baars, B. J. (1988). *A Cognitive Theory of Consciousness*. Cambridge University Press.
2. Tulving, E. (1983). *Elements of Episodic Memory*. Oxford University Press.
3. vLLM Documentation: https://docs.vllm.ai
4. LangGraph Documentation: https://langchain-ai.github.io/langgraph/

---

## License

MIT License - See [LICENSE](LICENSE) file for details.

---

## Contributing

This is a research/experimental project. For questions or improvements:

- **Issues:** [GitHub Issues](https://github.com/zoadrazorro/Zoadra_GWT_Engine/issues)
- **Documentation:** See `docs/` folder
- **Code Style:** Black formatting, Ruff linting

---

## Acknowledgments

- Built on [vLLM](https://github.com/vllm-project/vllm) for high-performance inference
- Orchestrated with [LangGraph](https://github.com/langchain-ai/langgraph)
- Distributed computing via [Ray](https://github.com/ray-project/ray)
- Inspired by Bernard Baars' Global Workspace Theory

---

**Status:** Alpha - Experimental consciousness simulation research project
**Hardware Target:** Dual AMD RX 7900 XT + Ryzen 9 7950X + 128GB RAM
**Performance:** 10-14 concurrent workers, 32-48 requests/sec, ~500ms integration latency
